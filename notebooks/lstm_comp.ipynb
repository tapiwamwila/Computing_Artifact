{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e4cef29-9370-4f34-8115-5476bfd0ccc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "\n",
    "import gcsfs\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "\n",
    "FILE_SYSTEM = gcsfs.core.GCSFileSystem(requester_pays=True)\n",
    "CAMELS_ROOT = Path('/Users/mpgrad/Desktop/PHD_Research/camels-20240903T2000Z/basin_timeseries_v1p2_metForcing_obsFlow 2/basin_dataset_public_v1p2')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "CAMELS_ROOT.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5463e413-3eb7-4c84-b2a8-d7d038190c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_forcing(basin: str) -> Tuple[pd.DataFrame, int]:\n",
    "    \"\"\"Load the meteorological forcing data of a specific basin.\n",
    "\n",
    "    :param basin: 8-digit code of basin as string.\n",
    "    :return: pd.DataFrame containing the meteorological forcing data and the area of the basin as integer.\n",
    "    \"\"\"\n",
    "    # Root directory of meteorological forcings\n",
    "    forcing_path = os.path.join(CAMELS_ROOT, 'basin_mean_forcing', 'daymet')\n",
    "\n",
    "    # path of forcing file\n",
    "    files = list(glob.glob(os.path.join(forcing_path, '**', f'{basin}_*.txt'), recursive=True))\n",
    "    if len(files) == 0:\n",
    "        raise RuntimeError(f'No forcing file found for Basin {basin}')\n",
    "    else:\n",
    "        file_path = files[0]\n",
    "\n",
    "    # converting date to datetime index and reading it\n",
    "    with open(file_path, 'r') as fp:\n",
    "        df = pd.read_csv(fp, sep='\\s+', header=3)\n",
    "    dates = df.Year.map(str) + \"/\" + df.Mnth.map(str) + \"/\" + df.Day.map(str)\n",
    "    df.index = pd.to_datetime(dates, format=\"%Y/%m/%d\")\n",
    "\n",
    "    # Load area from header\n",
    "    with open(file_path, 'r') as fp:\n",
    "        content = fp.readlines()\n",
    "        area = int(content[2])\n",
    "\n",
    "    return df, area\n",
    "\n",
    "\n",
    "def load_discharge(basin: str, area: int) ->  pd.Series:\n",
    "    \"\"\"Load the discharge time series for a specific basin.\n",
    "\n",
    "    :param basin: 8-digit code of basin as string.\n",
    "    :param area: int, area of the catchment in square meters\n",
    "    \n",
    "    :return: A pd.Series containng the catchment normalized discharge.\n",
    "    \"\"\"\n",
    "    # root directory of the streamflow data\n",
    "    discharge_path = CAMELS_ROOT  / 'usgs_streamflow'\n",
    "    \n",
    "    # get path of streamflow file file\n",
    "    files = list(FILE_SYSTEM.glob(f\"{str(discharge_path)}/**/{basin}_*.txt\"))\n",
    "    if len(files) == 0:\n",
    "        raise RuntimeError(f'No discharge file found for Basin {basin}')\n",
    "    else:\n",
    "        file_path = files[0]\n",
    "\n",
    "    # read-in data and convert date to datetime index\n",
    "    col_names = ['basin', 'Year', 'Mnth', 'Day', 'QObs', 'flag']\n",
    "    with FILE_SYSTEM.open(file_path) as fp:\n",
    "        df = pd.read_csv(fp, sep='\\s+', header=None, names=col_names)\n",
    "    dates = (df.Year.map(str) + \"/\" + df.Mnth.map(str) + \"/\"\n",
    "             + df.Day.map(str))\n",
    "    df.index = pd.to_datetime(dates, format=\"%Y/%m/%d\")\n",
    "\n",
    "    # normalize discharge from cubic feed per second to mm per day\n",
    "    df.QObs = 28316846.592 * df.QObs * 86400 / (area * 10 ** 6)\n",
    "\n",
    "    return df.QObs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ce669ff-d955-48fb-b4ed-a6894eac57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def reshape_data(x: np.ndarray, y: np.ndarray, seq_length: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Reshape matrix data into sample shape for LSTM training.\n",
    "\n",
    "    :param x: Matrix containing input features column wise and time steps row wise\n",
    "    :param y: Matrix containing the output feature.\n",
    "    :param seq_length: Length of look back days for one day of prediction\n",
    "    \n",
    "    :return: Two np.ndarrays, the first of shape (samples, length of sequence,\n",
    "        number of features), containing the input data for the LSTM. The second\n",
    "        of shape (samples, 1) containing the expected output for each input\n",
    "        sample.\n",
    "    \"\"\"\n",
    "    num_samples, num_features = x.shape\n",
    "\n",
    "    x_new = np.zeros((num_samples - seq_length + 1, seq_length, num_features))\n",
    "    y_new = np.zeros((num_samples - seq_length + 1, 1))\n",
    "\n",
    "    for i in range(0, x_new.shape[0]):\n",
    "        x_new[i, :, :num_features] = x[i:i + seq_length, :]\n",
    "        y_new[i, :] = y[i + seq_length - 1, 0]\n",
    "\n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e106b333-ca94-40b2-beea-5c7a61150b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamelsTXT(Dataset):\n",
    "    \"\"\"Torch Dataset for basic use of data from the CAMELS data set.\n",
    "\n",
    "    This data set provides meteorological observations and discharge of a given\n",
    "    basin from the CAMELS data set.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, basin: str, seq_length: int=365,period: str=None,\n",
    "                 dates: List=None, means: pd.Series=None, stds: pd.Series=None):\n",
    "        \"\"\"Initialize Dataset containing the data of a single basin.\n",
    "\n",
    "        :param basin: 8-digit code of basin as string.\n",
    "        :param seq_length: (optional) Length of the time window of\n",
    "            meteorological input provided for one time step of prediction.\n",
    "        :param period: (optional) One of ['train', 'eval']. None loads the \n",
    "            entire time series.\n",
    "        :param dates: (optional) List of pd.DateTimes of the start and end date \n",
    "            of the discharge period that is used.\n",
    "        :param means: (optional) Means of input and output features derived from\n",
    "            the training period. Has to be provided for 'eval' period. Can be\n",
    "            retrieved if calling .get_means() on the data set.\n",
    "        :param stds: (optional) Stds of input and output features derived from\n",
    "            the training period. Has to be provided for 'eval' period. Can be\n",
    "            retrieved if calling .get_stds() on the data set.\n",
    "        \"\"\"\n",
    "        self.basin = basin\n",
    "        self.seq_length = seq_length\n",
    "        self.period = period\n",
    "        self.dates = dates\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "\n",
    "        # load data into memory\n",
    "        self.x, self.y = self._load_data()\n",
    "\n",
    "        # store number of samples as class attribute\n",
    "        self.num_samples = self.x.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Load input and output data from text files.\"\"\"\n",
    "        df, area = load_forcing(self.basin)\n",
    "        df['QObs(mm/d)'] = load_discharge(self.basin, area)\n",
    "        \n",
    "        if self.dates is not None:\n",
    "            # If meteorological observations exist before start date\n",
    "            # use these as well. Similiar to hydrological warmup period.\n",
    "            if self.dates[0] - pd.DateOffset(days=self.seq_length) > df.index[0]:\n",
    "                start_date = self.dates[0] - pd.DateOffset(days=self.seq_length)\n",
    "            else:\n",
    "                start_date = self.dates[0]\n",
    "            df = df[start_date:self.dates[1]]\n",
    "\n",
    "        # if training period store means and stds\n",
    "        if self.period == 'train':\n",
    "            self.means = df.mean()\n",
    "            self.stds = df.std()\n",
    "\n",
    "        # extract input and output features from DataFrame\n",
    "        x = np.array([df['prcp(mm/day)'].values,\n",
    "                      df['srad(W/m2)'].values,\n",
    "                      df['tmax(C)'].values,\n",
    "                      df['tmin(C)'].values,\n",
    "                      df['vp(Pa)'].values]).T\n",
    "        y = np.array([df['QObs(mm/d)'].values]).T\n",
    "\n",
    "        # normalize data, reshape for LSTM training and remove invalid samples\n",
    "        x = self._local_normalization(x, variable='inputs')\n",
    "        x, y = reshape_data(x, y, self.seq_length)\n",
    "\n",
    "        if self.period == \"train\":\n",
    "            # Delete all samples, where discharge is NaN\n",
    "            if np.sum(np.isnan(y)) > 0:\n",
    "                print(f\"Deleted some records because of NaNs {self.basin}\")\n",
    "                x = np.delete(x, np.argwhere(np.isnan(y)), axis=0)\n",
    "                y = np.delete(y, np.argwhere(np.isnan(y)), axis=0)\n",
    "            \n",
    "            # Deletes all records, where no discharge was measured (-999)\n",
    "            x = np.delete(x, np.argwhere(y < 0)[:, 0], axis=0)\n",
    "            y = np.delete(y, np.argwhere(y < 0)[:, 0], axis=0)\n",
    "            \n",
    "            # normalize discharge\n",
    "            y = self._local_normalization(y, variable='output')\n",
    "\n",
    "        # convert arrays to torch tensors\n",
    "        x = torch.from_numpy(x.astype(np.float32))\n",
    "        y = torch.from_numpy(y.astype(np.float32))\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def _local_normalization(self, feature: np.ndarray, variable: str) -> \\\n",
    "            np.ndarray:\n",
    "        \"\"\"Normalize input/output features with local mean/std.\n",
    "\n",
    "        :param feature: Numpy array containing the feature(s) as matrix.\n",
    "        :param variable: Either 'inputs' or 'output' showing which feature will\n",
    "            be normalized\n",
    "        :return: array containing the normalized feature\n",
    "        \"\"\"\n",
    "        if variable == 'inputs':\n",
    "            means = np.array([self.means['prcp(mm/day)'],\n",
    "                              self.means['srad(W/m2)'],\n",
    "                              self.means['tmax(C)'],\n",
    "                              self.means['tmin(C)'],\n",
    "                              self.means['vp(Pa)']])\n",
    "            stds = np.array([self.stds['prcp(mm/day)'],\n",
    "                             self.stds['srad(W/m2)'],\n",
    "                             self.stds['tmax(C)'],\n",
    "                             self.stds['tmin(C)'],\n",
    "                             self.stds['vp(Pa)']])\n",
    "            feature = (feature - means) / stds\n",
    "        elif variable == 'output':\n",
    "            feature = ((feature - self.means[\"QObs(mm/d)\"]) /\n",
    "                       self.stds[\"QObs(mm/d)\"])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unknown variable type {variable}\")\n",
    "\n",
    "        return feature\n",
    "\n",
    "    def local_rescale(self, feature: np.ndarray, variable: str) -> \\\n",
    "            np.ndarray:\n",
    "        \"\"\"Rescale input/output features with local mean/std.\n",
    "\n",
    "        :param feature: Numpy array containing the feature(s) as matrix.\n",
    "        :param variable: Either 'inputs' or 'output' showing which feature will\n",
    "            be normalized\n",
    "        :return: array containing the normalized feature\n",
    "        \"\"\"\n",
    "        if variable == 'inputs':\n",
    "            means = np.array([self.means['prcp(mm/day)'],\n",
    "                              self.means['srad(W/m2)'],\n",
    "                              self.means['tmax(C)'],\n",
    "                              self.means['tmin(C)'],\n",
    "                              self.means['vp(Pa)']])\n",
    "            stds = np.array([self.stds['prcp(mm/day)'],\n",
    "                             self.stds['srad(W/m2)'],\n",
    "                             self.stds['tmax(C)'],\n",
    "                             self.stds['tmin(C)'],\n",
    "                             self.stds['vp(Pa)']])\n",
    "            feature = feature * stds + means\n",
    "        elif variable == 'output':\n",
    "            feature = (feature * self.stds[\"QObs(mm/d)\"] +\n",
    "                       self.means[\"QObs(mm/d)\"])\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unknown variable type {variable}\")\n",
    "\n",
    "        return feature\n",
    "\n",
    "    def get_means(self):\n",
    "        return self.means\n",
    "\n",
    "    def get_stds(self):\n",
    "        return self.stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47475b70-78fa-442f-9ddf-1ef700dcef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"Implementation of a single layer LSTM network\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, dropout_rate: float=0.0):\n",
    "        \"\"\"Initialize model\n",
    "        \n",
    "        :param hidden_size: Number of hidden units/LSTM cells\n",
    "        :param dropout_rate: Dropout rate of the last fully connected\n",
    "            layer. Default 0.0\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # create required layer\n",
    "        self.lstm = nn.LSTM(input_size=5, hidden_size=self.hidden_size, \n",
    "                            num_layers=1, bias=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "        self.fc = nn.Linear(in_features=self.hidden_size, out_features=1)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the Network.\n",
    "        \n",
    "        :param x: Tensor of shape [batch size, seq length, num features]\n",
    "            containing the input data for the LSTM network.\n",
    "        \n",
    "        :return: Tensor containing the network predictions\n",
    "        \"\"\"\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # perform prediction only at the end of the input sequence\n",
    "        pred = self.fc(self.dropout(h_n[-1,:,:]))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8500746-0a23-42a5-94ea-2930b9e162b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, loader, loss_func, epoch):\n",
    "    \"\"\"Train model for a single epoch.\n",
    "\n",
    "    :param model: A torch.nn.Module implementing the LSTM model\n",
    "    :param optimizer: One of PyTorchs optimizer classes.\n",
    "    :param loader: A PyTorch DataLoader, providing the trainings\n",
    "        data in mini batches.\n",
    "    :param loss_func: The loss function to minimize.\n",
    "    :param epoch: The current epoch (int) used for the progress bar\n",
    "    \"\"\"\n",
    "    # set model to train mode (important for dropout)\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm_notebook(loader)\n",
    "    pbar.set_description(f\"Epoch {epoch}\")\n",
    "    # request mini-batch of data from the loader\n",
    "    for xs, ys in pbar:\n",
    "        # delete previously stored gradients from the model\n",
    "        optimizer.zero_grad()\n",
    "        # push data to GPU (if available)\n",
    "        xs, ys = xs.to(DEVICE), ys.to(DEVICE)\n",
    "        # get model predictions\n",
    "        y_hat = model(xs)\n",
    "        # calculate loss\n",
    "        loss = loss_func(y_hat, ys)\n",
    "        # calculate gradients\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        # write current loss in the progress bar\n",
    "        pbar.set_postfix_str(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        \n",
    "def eval_model(model, loader) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Evaluate the model.\n",
    "\n",
    "    :param model: A torch.nn.Module implementing the LSTM model\n",
    "    :param loader: A PyTorch DataLoader, providing the data.\n",
    "    \n",
    "    :return: Two torch Tensors, containing the observations and \n",
    "        model predictions\n",
    "    \"\"\"\n",
    "    # set model to eval mode (important for dropout)\n",
    "    model.eval()\n",
    "    obs = []\n",
    "    preds = []\n",
    "    # in inference mode, we don't need to store intermediate steps for\n",
    "    # backprob\n",
    "    with torch.no_grad():\n",
    "        # request mini-batch of data from the loader\n",
    "        for xs, ys in loader:\n",
    "            # push data to GPU (if available)\n",
    "            xs = xs.to(DEVICE)\n",
    "            # get model predictions\n",
    "            y_hat = model(xs)\n",
    "            obs.append(ys)\n",
    "            preds.append(y_hat)\n",
    "            \n",
    "    return torch.cat(obs), torch.cat(preds)\n",
    "        \n",
    "def calc_nse(obs: np.array, sim: np.array) -> float:\n",
    "    \"\"\"Calculate Nash-Sutcliff-Efficiency.\n",
    "\n",
    "    :param obs: Array containing the observations\n",
    "    :param sim: Array containing the simulations\n",
    "    :return: NSE value.\n",
    "    \"\"\"\n",
    "    # only consider time steps, where observations are available\n",
    "    sim = np.delete(sim, np.argwhere(obs < 0), axis=0)\n",
    "    obs = np.delete(obs, np.argwhere(obs < 0), axis=0)\n",
    "\n",
    "    # check for NaNs in observations\n",
    "    sim = np.delete(sim, np.argwhere(np.isnan(obs)), axis=0)\n",
    "    obs = np.delete(obs, np.argwhere(np.isnan(obs)), axis=0)\n",
    "\n",
    "    denominator = np.sum((obs - np.mean(obs)) ** 2)\n",
    "    numerator = np.sum((sim - obs) ** 2)\n",
    "    nse_val = 1 - numerator / denominator\n",
    "\n",
    "    return nse_val\n",
    "\n",
    "def extract_gage_ids(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    last_word = df['GAUGE_NAME'].str.extract(r'\\s*(\\w+)$')[0].str.upper()\n",
    "    matches = df[last_word == 'ID']\n",
    "    return matches['GAGE_ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "58b5a0e5-df14-49a9-b21e-f1b439c0efb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12411000, 12414500, 13083000, 13235000, 13240000, 13310700, 13313000, 13337000, 13338500, 13340000, 13340600]\n"
     ]
    }
   ],
   "source": [
    "def extract_gage_ids(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    last_word = df['GAUGE_NAME'].str.extract(r'\\s*(\\w+)$')[0].str.upper()\n",
    "    matches = df[last_word == 'ID']\n",
    "    return matches['GAGE_ID'].tolist()\n",
    "\n",
    "file_path = 'gauge_information.xlsx'\n",
    "basin_ID = extract_gage_ids(file_path)\n",
    "print(basin_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb24927d-74c5-411d-898b-7278da98efce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running model for basin 12411000 =====\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "b/Users/o",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m start_date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1980-10-01\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m end_date \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1995-09-30\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m ds_train \u001b[38;5;241m=\u001b[39m CamelsTXT(basin, seq_length\u001b[38;5;241m=\u001b[39msequence_length, period\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, dates\u001b[38;5;241m=\u001b[39m[start_date, end_date])\n\u001b[1;32m     19\u001b[0m tr_loader \u001b[38;5;241m=\u001b[39m DataLoader(ds_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m means \u001b[38;5;241m=\u001b[39m ds_train\u001b[38;5;241m.\u001b[39mget_means()\n",
      "Cell \u001b[0;32mIn[37], line 34\u001b[0m, in \u001b[0;36mCamelsTXT.__init__\u001b[0;34m(self, basin, seq_length, period, dates, means, stds)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstds \u001b[38;5;241m=\u001b[39m stds\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# load data into memory\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_data()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# store number of samples as class attribute\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[37], line 48\u001b[0m, in \u001b[0;36mCamelsTXT._load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load input and output data from text files.\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m df, area \u001b[38;5;241m=\u001b[39m load_forcing(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasin)\n\u001b[0;32m---> 48\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQObs(mm/d)\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m load_discharge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasin, area)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdates \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# If meteorological observations exist before start date\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# use these as well. Similiar to hydrological warmup period.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdates[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m pd\u001b[38;5;241m.\u001b[39mDateOffset(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_length) \u001b[38;5;241m>\u001b[39m df\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m0\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[56], line 43\u001b[0m, in \u001b[0;36mload_discharge\u001b[0;34m(basin, area)\u001b[0m\n\u001b[1;32m     40\u001b[0m discharge_path \u001b[38;5;241m=\u001b[39m CAMELS_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124musgs_streamflow\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# get path of streamflow file file\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(FILE_SYSTEM\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(discharge_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/**/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbasin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_*.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(files) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo discharge file found for Basin \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbasin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fsspec/asyn.py:103\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fsspec/asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     54\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     58\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fsspec/asyn.py:804\u001b[0m, in \u001b[0;36mAsyncFileSystem._glob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    802\u001b[0m         depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 804\u001b[0m allpaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find(\n\u001b[1;32m    805\u001b[0m     root, maxdepth\u001b[38;5;241m=\u001b[39mdepth, withdirs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    806\u001b[0m )\n\u001b[1;32m    808\u001b[0m pattern \u001b[38;5;241m=\u001b[39m glob_translate(path \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ends_with_sep \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    809\u001b[0m pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(pattern)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gcsfs/core.py:1453\u001b[0m, in \u001b[0;36mGCSFileSystem._find\u001b[0;34m(self, path, withdirs, detail, prefix, versions, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxdepth must be at least 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;66;03m# Fetch objects as if the path is a directory\u001b[39;00m\n\u001b[0;32m-> 1453\u001b[0m objects, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_list_objects(\n\u001b[1;32m   1454\u001b[0m     path, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, prefix\u001b[38;5;241m=\u001b[39mprefix, versions\u001b[38;5;241m=\u001b[39mversions\n\u001b[1;32m   1455\u001b[0m )\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m objects:\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;66;03m# Fetch objects as if the path is a file\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m     bucket, key, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_path(path)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gcsfs/core.py:649\u001b[0m, in \u001b[0;36mGCSFileSystem._do_list_objects\u001b[0;34m(self, path, max_results, delimiter, prefix, versions, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concurrent_list_objects_helper(\n\u001b[1;32m    638\u001b[0m         items\u001b[38;5;241m=\u001b[39mitems,\n\u001b[1;32m    639\u001b[0m         bucket\u001b[38;5;241m=\u001b[39mbucket,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m         page_size\u001b[38;5;241m=\u001b[39mdefault_page_size,\n\u001b[1;32m    644\u001b[0m     )\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# If the user has not configured inventory report, proceed to use\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# sequential listing.\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sequential_list_objects_helper(\n\u001b[1;32m    650\u001b[0m         bucket\u001b[38;5;241m=\u001b[39mbucket,\n\u001b[1;32m    651\u001b[0m         delimiter\u001b[38;5;241m=\u001b[39mdelimiter,\n\u001b[1;32m    652\u001b[0m         start_offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    653\u001b[0m         end_offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    654\u001b[0m         prefix\u001b[38;5;241m=\u001b[39mprefix,\n\u001b[1;32m    655\u001b[0m         versions\u001b[38;5;241m=\u001b[39mversions,\n\u001b[1;32m    656\u001b[0m         page_size\u001b[38;5;241m=\u001b[39mdefault_page_size,\n\u001b[1;32m    657\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gcsfs/core.py:743\u001b[0m, in \u001b[0;36mGCSFileSystem._sequential_list_objects_helper\u001b[0;34m(self, bucket, delimiter, start_offset, end_offset, prefix, versions, page_size)\u001b[0m\n\u001b[1;32m    740\u001b[0m prefixes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    741\u001b[0m items \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 743\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/o\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    746\u001b[0m     bucket,\n\u001b[1;32m    747\u001b[0m     delimiter\u001b[38;5;241m=\u001b[39mdelimiter,\n\u001b[1;32m    748\u001b[0m     prefix\u001b[38;5;241m=\u001b[39mprefix,\n\u001b[1;32m    749\u001b[0m     startOffset\u001b[38;5;241m=\u001b[39mstart_offset,\n\u001b[1;32m    750\u001b[0m     endOffset\u001b[38;5;241m=\u001b[39mend_offset,\n\u001b[1;32m    751\u001b[0m     maxResults\u001b[38;5;241m=\u001b[39mpage_size,\n\u001b[1;32m    752\u001b[0m     json_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    753\u001b[0m     versions\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m versions \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    754\u001b[0m )\n\u001b[1;32m    756\u001b[0m prefixes\u001b[38;5;241m.\u001b[39mextend(page\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefixes\u001b[39m\u001b[38;5;124m\"\u001b[39m, []))\n\u001b[1;32m    757\u001b[0m items\u001b[38;5;241m.\u001b[39mextend(page\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m, []))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gcsfs/core.py:447\u001b[0m, in \u001b[0;36mGCSFileSystem._call\u001b[0;34m(self, method, path, json_out, info_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, path, \u001b[38;5;241m*\u001b[39margs, json_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, info_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    445\u001b[0m ):\n\u001b[1;32m    446\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 447\u001b[0m     status, headers, info, contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    448\u001b[0m         method, path, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    449\u001b[0m     )\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m json_out:\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(contents)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/decorator.py:221\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    220\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gcsfs/retry.py:126\u001b[0m, in \u001b[0;36mretry_request\u001b[0;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mmin\u001b[39m(random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (retry \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m32\u001b[39m))\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    128\u001b[0m     HttpError,\n\u001b[1;32m    129\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m     aiohttp\u001b[38;5;241m.\u001b[39mclient_exceptions\u001b[38;5;241m.\u001b[39mClientError,\n\u001b[1;32m    133\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(e, HttpError)\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequester pays\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    138\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gcsfs/core.py:440\u001b[0m, in \u001b[0;36mGCSFileSystem._request\u001b[0;34m(self, method, path, headers, json, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m info \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mrequest_info  \u001b[38;5;66;03m# for debug only\u001b[39;00m\n\u001b[1;32m    438\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m r\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 440\u001b[0m validate_response(status, contents, path, args)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status, headers, info, contents\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gcsfs/retry.py:95\u001b[0m, in \u001b[0;36mvalidate_response\u001b[0;34m(status, content, path, args)\u001b[0m\n\u001b[1;32m     93\u001b[0m     path \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m[quote(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m args])\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[1;32m     97\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(content, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: b/Users/o"
     ]
    }
   ],
   "source": [
    "file_path = 'gauge_information.xlsx'\n",
    "basin_ids = extract_gage_ids(file_path)\n",
    "\n",
    "hidden_size = 10\n",
    "dropout_rate = 0.0\n",
    "learning_rate = 1e-3\n",
    "sequence_length = 365\n",
    "n_epochs = 20\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "for basin in basin_ids:\n",
    "    print(f\"\\n=== Running model for basin {basin} =====\")\n",
    "\n",
    "    start_date = pd.to_datetime(\"1980-10-01\")\n",
    "    end_date = pd.to_datetime(\"1995-09-30\")\n",
    "    ds_train = CamelsTXT(basin, seq_length=sequence_length, period=\"train\", dates=[start_date, end_date])\n",
    "    tr_loader = DataLoader(ds_train, batch_size=256, shuffle=True)\n",
    "\n",
    "    means = ds_train.get_means()\n",
    "    stds = ds_train.get_stds()\n",
    "\n",
    "    start_date = pd.to_datetime(\"1995-10-01\")\n",
    "    end_date = pd.to_datetime(\"2000-09-30\")\n",
    "    ds_val = CamelsTXT(basin, seq_length=sequence_length, period=\"eval\", dates=[start_date, end_date],\n",
    "                       means=means, stds=stds)\n",
    "    val_loader = DataLoader(ds_val, batch_size=2048, shuffle=False)\n",
    "\n",
    "    start_date = pd.to_datetime(\"2000-10-01\")\n",
    "    end_date = pd.to_datetime(\"2010-09-30\")\n",
    "    ds_test = CamelsTXT(basin, seq_length=sequence_length, period=\"eval\", dates=[start_date, end_date],\n",
    "                        means=means, stds=stds)\n",
    "    test_loader = DataLoader(ds_test, batch_size=2048, shuffle=False)\n",
    "\n",
    "    model = Model(hidden_size=hidden_size, dropout_rate=dropout_rate).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        train_epoch(model, optimizer, tr_loader, loss_func, i+1)\n",
    "        obs, preds = eval_model(model, val_loader)\n",
    "        preds = ds_val.local_rescale(preds.numpy(), variable='output')\n",
    "        nse = calc_nse(obs.numpy(), preds)\n",
    "        tqdm.tqdm.write(f\"Basin {basin} - Epoch {i+1} - Validation NSE: {nse:.2f}\")\n",
    "\n",
    "    obs, preds = eval_model(model, test_loader)\n",
    "    preds = ds_val.local_rescale(preds.numpy(), variable='output')\n",
    "    obs = obs.numpy()\n",
    "    nse = calc_nse(obs, preds)\n",
    "\n",
    "    start_date = ds_test.dates[0]\n",
    "    end_date = ds_test.dates[1] + pd.DateOffset(days=1)\n",
    "    date_range = pd.date_range(start_date, end_date)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(date_range, obs, label=\"observation\")\n",
    "    ax.plot(date_range, preds, label=\"prediction\")\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"Basin {basin} - Test set NSE: {nse:.3f}\")\n",
    "    ax.xaxis.set_tick_params(rotation=90)\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Discharge (mm/d)\")\n",
    "\n",
    "    fig_path = os.path.join(\"results\", f\"{basin}_test_plot.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved plot to {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b866568d-c254-41fb-862d-ff12a563f1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd236d-3f42-4fcd-9164-267e2a835988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c03c72-63a7-40a5-ba27-0b2ce189b4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "408e008f-a244-4296-b22b-1ff274573042",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "b/Users/o",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m load_forcing(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m12411000\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 13\u001b[0m, in \u001b[0;36mload_forcing\u001b[0;34m(basin)\u001b[0m\n\u001b[1;32m     10\u001b[0m forcing_path \u001b[38;5;241m=\u001b[39m CAMELS_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasin_mean_forcing\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaymet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# get path of forcing file\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(FILE_SYSTEM\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(forcing_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/**/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbasin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_*.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(files) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo forcing file file found for Basin \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbasin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fsspec/asyn.py:103\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fsspec/asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     54\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     58\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fsspec/asyn.py:804\u001b[0m, in \u001b[0;36mAsyncFileSystem._glob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    802\u001b[0m         depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 804\u001b[0m allpaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find(\n\u001b[1;32m    805\u001b[0m     root, maxdepth\u001b[38;5;241m=\u001b[39mdepth, withdirs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    806\u001b[0m )\n\u001b[1;32m    808\u001b[0m pattern \u001b[38;5;241m=\u001b[39m glob_translate(path \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ends_with_sep \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    809\u001b[0m pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(pattern)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gcsfs/core.py:1453\u001b[0m, in \u001b[0;36mGCSFileSystem._find\u001b[0;34m(self, path, withdirs, detail, prefix, versions, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxdepth must be at least 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;66;03m# Fetch objects as if the path is a directory\u001b[39;00m\n\u001b[0;32m-> 1453\u001b[0m objects, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_list_objects(\n\u001b[1;32m   1454\u001b[0m     path, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, prefix\u001b[38;5;241m=\u001b[39mprefix, versions\u001b[38;5;241m=\u001b[39mversions\n\u001b[1;32m   1455\u001b[0m )\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m objects:\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;66;03m# Fetch objects as if the path is a file\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m     bucket, key, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_path(path)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gcsfs/core.py:649\u001b[0m, in \u001b[0;36mGCSFileSystem._do_list_objects\u001b[0;34m(self, path, max_results, delimiter, prefix, versions, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concurrent_list_objects_helper(\n\u001b[1;32m    638\u001b[0m         items\u001b[38;5;241m=\u001b[39mitems,\n\u001b[1;32m    639\u001b[0m         bucket\u001b[38;5;241m=\u001b[39mbucket,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m         page_size\u001b[38;5;241m=\u001b[39mdefault_page_size,\n\u001b[1;32m    644\u001b[0m     )\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# If the user has not configured inventory report, proceed to use\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# sequential listing.\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sequential_list_objects_helper(\n\u001b[1;32m    650\u001b[0m         bucket\u001b[38;5;241m=\u001b[39mbucket,\n\u001b[1;32m    651\u001b[0m         delimiter\u001b[38;5;241m=\u001b[39mdelimiter,\n\u001b[1;32m    652\u001b[0m         start_offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    653\u001b[0m         end_offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    654\u001b[0m         prefix\u001b[38;5;241m=\u001b[39mprefix,\n\u001b[1;32m    655\u001b[0m         versions\u001b[38;5;241m=\u001b[39mversions,\n\u001b[1;32m    656\u001b[0m         page_size\u001b[38;5;241m=\u001b[39mdefault_page_size,\n\u001b[1;32m    657\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gcsfs/core.py:743\u001b[0m, in \u001b[0;36mGCSFileSystem._sequential_list_objects_helper\u001b[0;34m(self, bucket, delimiter, start_offset, end_offset, prefix, versions, page_size)\u001b[0m\n\u001b[1;32m    740\u001b[0m prefixes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    741\u001b[0m items \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 743\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/o\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    746\u001b[0m     bucket,\n\u001b[1;32m    747\u001b[0m     delimiter\u001b[38;5;241m=\u001b[39mdelimiter,\n\u001b[1;32m    748\u001b[0m     prefix\u001b[38;5;241m=\u001b[39mprefix,\n\u001b[1;32m    749\u001b[0m     startOffset\u001b[38;5;241m=\u001b[39mstart_offset,\n\u001b[1;32m    750\u001b[0m     endOffset\u001b[38;5;241m=\u001b[39mend_offset,\n\u001b[1;32m    751\u001b[0m     maxResults\u001b[38;5;241m=\u001b[39mpage_size,\n\u001b[1;32m    752\u001b[0m     json_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    753\u001b[0m     versions\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m versions \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    754\u001b[0m )\n\u001b[1;32m    756\u001b[0m prefixes\u001b[38;5;241m.\u001b[39mextend(page\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefixes\u001b[39m\u001b[38;5;124m\"\u001b[39m, []))\n\u001b[1;32m    757\u001b[0m items\u001b[38;5;241m.\u001b[39mextend(page\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m, []))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gcsfs/core.py:447\u001b[0m, in \u001b[0;36mGCSFileSystem._call\u001b[0;34m(self, method, path, json_out, info_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, path, \u001b[38;5;241m*\u001b[39margs, json_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, info_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    445\u001b[0m ):\n\u001b[1;32m    446\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 447\u001b[0m     status, headers, info, contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    448\u001b[0m         method, path, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    449\u001b[0m     )\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m json_out:\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(contents)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/decorator.py:221\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    220\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gcsfs/retry.py:126\u001b[0m, in \u001b[0;36mretry_request\u001b[0;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mmin\u001b[39m(random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (retry \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m32\u001b[39m))\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    128\u001b[0m     HttpError,\n\u001b[1;32m    129\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m     aiohttp\u001b[38;5;241m.\u001b[39mclient_exceptions\u001b[38;5;241m.\u001b[39mClientError,\n\u001b[1;32m    133\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(e, HttpError)\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequester pays\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    138\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gcsfs/core.py:440\u001b[0m, in \u001b[0;36mGCSFileSystem._request\u001b[0;34m(self, method, path, headers, json, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m info \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mrequest_info  \u001b[38;5;66;03m# for debug only\u001b[39;00m\n\u001b[1;32m    438\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m r\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 440\u001b[0m validate_response(status, contents, path, args)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status, headers, info, contents\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gcsfs/retry.py:95\u001b[0m, in \u001b[0;36mvalidate_response\u001b[0;34m(status, content, path, args)\u001b[0m\n\u001b[1;32m     93\u001b[0m     path \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m[quote(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m args])\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[1;32m     97\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(content, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: b/Users/o"
     ]
    }
   ],
   "source": [
    "load_forcing('12411000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "287de4f1-420b-4813-a33b-67252968b76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['readme_hru_forcing.txt',\n",
       " 'usgs_streamflow',\n",
       " 'readme_shapefiles.txt',\n",
       " '.DS_Store',\n",
       " 'readme_streamflow.txt',\n",
       " 'shapefiles',\n",
       " 'basin_metadata',\n",
       " 'readme_model_output.txt',\n",
       " 'elev_bands_forcing',\n",
       " 'hru_forcing',\n",
       " 'readme_FIRST.txt',\n",
       " 'basin_mean_forcing',\n",
       " 'dataset_summary.txt',\n",
       " 'readme_elev_bands_forcing.txt',\n",
       " 'basin_size_errors_10_percent.txt',\n",
       " 'readme_basin_mean_forcing.txt']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(CAMELS_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc3c6d3b-40c1-4866-9c36-943baa5e6edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['03',\n",
       " '04',\n",
       " '05',\n",
       " '02',\n",
       " '.DS_Store',\n",
       " '18',\n",
       " '11',\n",
       " '16',\n",
       " '17',\n",
       " '10',\n",
       " '07',\n",
       " '09',\n",
       " '08',\n",
       " '01',\n",
       " '06',\n",
       " '15',\n",
       " '12',\n",
       " '13',\n",
       " '14']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discharge_path=os.path.join(CAMELS_ROOT,'usgs_streamflow')\n",
    "os.listdir(discharge_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc1f4dcd-0f1a-4b36-96e4-a82108da1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "23a5274a-68dc-4f46-8a33-ba7aa2daadcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(            Year  Mnth  Day  Hr   dayl(s)  prcp(mm/day)  srad(W/m2)  swe(mm)  \\\n",
       " 1980-01-01  1980     1    1  12  29721.59          6.33       37.15      0.0   \n",
       " 1980-01-02  1980     1    2  12  29730.20          3.06       69.25      0.0   \n",
       " 1980-01-03  1980     1    3  12  29846.94          3.50      102.97      0.0   \n",
       " 1980-01-04  1980     1    4  12  29991.64          1.02      139.95      0.0   \n",
       " 1980-01-05  1980     1    5  12  30067.19         17.26      120.24      0.0   \n",
       " ...          ...   ...  ...  ..       ...           ...         ...      ...   \n",
       " 2014-12-27  2014    12   27  12  29501.34          0.00       97.21      0.0   \n",
       " 2014-12-28  2014    12   28  12  29573.02         18.46       97.39      0.0   \n",
       " 2014-12-29  2014    12   29  12  29700.37         10.02      134.40      0.0   \n",
       " 2014-12-30  2014    12   30  12  29721.58          0.00      187.15      0.0   \n",
       " 2014-12-31  2014    12   31  12  29721.59          0.00      168.41      0.0   \n",
       " \n",
       "             tmax(C)  tmin(C)  vp(Pa)  \n",
       " 1980-01-01     2.13    -0.35  597.17  \n",
       " 1980-01-02     2.80    -2.08  527.68  \n",
       " 1980-01-03    -1.14    -8.61  321.42  \n",
       " 1980-01-04    -4.82   -12.84  229.86  \n",
       " 1980-01-05    -0.92   -10.08  286.12  \n",
       " ...             ...      ...     ...  \n",
       " 2014-12-27    -1.67    -5.42  412.46  \n",
       " 2014-12-28    -1.96    -7.10  358.83  \n",
       " 2014-12-29    -6.17   -14.87  194.85  \n",
       " 2014-12-30    -8.82   -18.66  143.28  \n",
       " 2014-12-31    -8.72   -15.96  170.21  \n",
       " \n",
       " [12784 rows x 11 columns],\n",
       " 867275135)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_forcing('12411000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a4a00a-68b7-48a5-8fe2-cb247f59170c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
